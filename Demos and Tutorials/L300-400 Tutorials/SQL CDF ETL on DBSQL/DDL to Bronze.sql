-- Databricks notebook source
-- DBTITLE 1,Use Dynamic SQL to dynamically truncate and reload
DECLARE OR REPLACE VARIABLE start_over_sql STRING;

SET VARIABLE start_over_sql = (SELECT 
                                    MAX(CASE WHEN "${ReloadFromScratch}" = 'no' THEN "SELECT 'Running incrmentally from last state' AS status" ELSE "DROP SCHEMA IF EXISTS IDENTIFIER('$Schema') CASCADE" END) AS stmt
);

EXECUTE IMMEDIATE start_over_sql;

-- COMMAND ----------

USE CATALOG '${Catalog}';
CREATE SCHEMA IF NOT EXISTS IDENTIFIER('${Schema}');
USE IDENTIFIER(CONCAT('${Catalog}','.', '${Schema}')); -- this is the only combo that work at schema level. USE SCHEMA does not like dynamic

-- COMMAND ----------

-- DBTITLE 1,Confirm Scope
SELECT current_catalog(), current_database()

-- COMMAND ----------

-- DBTITLE 1,Define DDL
--===== Step 1 - Create Bronze Streaming Table
CREATE TABLE IF NOT EXISTS cdf_demo_bronze_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
event_timestamp TIMESTAMP,
value STRING,
bronze_update_timestamp TIMESTAMP
)
USING DELTA
TBLPROPERTIES ('delta.targetFileSize' = '1mb', 'delta.feature.allowColumnDefaults' = 'supported', 'delta.columnMapping.mode' = 'name', 'delta.enableChangeDataFeed'= 'true')
CLUSTER BY (Id, user_id, device_id)
;

---===== Step 2 - Create Silver Target Table
CREATE TABLE IF NOT EXISTS cdf_demo_silver_sensors
(
    Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
    device_id INT,
    user_id INT,
    calories_burnt DECIMAL(10,2), 
    miles_walked DECIMAL(10,2), 
    num_steps DECIMAL(10,2), 
    event_timestamp TIMESTAMP,
    value STRING,
    silver_update_timestamp TIMESTAMP,
    last_version INT -- Can either be tracked in the table or separately
)
TBLPROPERTIES ('delta.targetFileSize' = '1mb', 'delta.feature.allowColumnDefaults' = 'supported', 'delta.columnMapping.mode' = 'name', 'delta.enableChangeDataFeed'= 'true')
CLUSTER BY (Id, user_id, device_id)
;


---===== Step 2 - Create Silver Target Table
CREATE TABLE IF NOT EXISTS cdf_demo_silver_sensors_scd_2
(
    Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
    device_id INT,
    user_id INT,
    calories_burnt DECIMAL(10,2), 
    miles_walked DECIMAL(10,2), 
    num_steps DECIMAL(10,2), 
    event_timestamp TIMESTAMP,
    value STRING,
    is_active BOOLEAN,
    start_timestamp INT,
    end_timestamp INT,
    silver_update_timestamp TIMESTAMP,
    last_version INT -- Can either be tracked in the table or separately
)
TBLPROPERTIES ('delta.targetFileSize' = '1mb', 'delta.feature.allowColumnDefaults' = 'supported', 'delta.columnMapping.mode' = 'name', 'delta.enableChangeDataFeed'= 'true')
CLUSTER BY (Id, user_id, device_id);


--TIP:  Not always necessary depending on your update patterns, can track this directly in the silver table as well, just might be faster and more reliable if track in a separate table

-- COMMAND ----------

-- DBTITLE 1,Define Version Checkpoints

CREATE TABLE IF NOT EXISTS cdf_checkpoint_silver_sensor (latest_version INT DEFAULT 0, update_timestamp TIMESTAMP)
TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')
CLUSTER BY (latest_version)
;

CREATE TABLE IF NOT EXISTS cdf_checkpoint_gold_users (latest_version INT DEFAULT 0, update_timestamp TIMESTAMP)
TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')
CLUSTER BY (latest_version)
;

CREATE TABLE IF NOT EXISTS cdf_checkpoint_gold_users (latest_version INT DEFAULT 0, update_timestamp TIMESTAMP)
TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')
CLUSTER BY (latest_version)
;

CREATE TABLE IF NOT EXISTS cdf_checkpoint_gold_devices (latest_version INT DEFAULT 0, update_timestamp TIMESTAMP)
TBLPROPERTIES('delta.feature.allowColumnDefaults' = 'supported')
CLUSTER BY (latest_version)
;

-- COMMAND ----------

-- DBTITLE 1,Run COPY INTO - Raw to Bronze
-- Can do graceful data type conversion directly in the COPY statement
-- COPY OPTIONS for robust handling of changes to source data / bronze tables

COPY INTO cdf_demo_bronze_sensors
FROM (SELECT 
      id::bigint AS Id,
      device_id::integer AS device_id,
      user_id::integer AS user_id,
      calories_burnt::decimal(10,2) AS calories_burnt, 
      miles_walked::decimal(10,2) AS miles_walked, 
      num_steps::decimal(10,2) AS num_steps, 
      timestamp::timestamp AS event_timestamp,
      value  AS value, -- This is a JSON object,
      now() AS bronze_update_timestamp
FROM "/databricks-datasets/iot-stream/data-device/")
FILEFORMAT = json -- csv, xml, txt, parquet, binary, etc.
COPY_OPTIONS('force'='true', 'ignoreChanges' = 'true', 'ignoreDeletes' = 'true', 'mergeSchema' = 'true') --'true' always loads all data it sees. option to be incremental or always load all files


--Other Helpful copy options:
/*
PATTERN('[A-Za-z0-9].json') --Only load files that match a regex pattern
FORMAT_OPTIONS ('ignoreCorruptFiles' = 'true') -- skips bad files for more robust incremental loads
COPY_OPTIONS ('mergeSchema' = 'true') --auto evolve the schema at the bronze layer
'ignoreChanges' = 'true' - ENSURE DOWNSTREAM PIPELINE CAN HANDLE DUPLICATE ALREADY PROCESSED RECORDS WITH MERGE/INSERT WHERE NOT EXISTS/Etc.
'ignoreDeletes' = 'true'
*/;

-- COMMAND ----------

-- DBTITLE 1,Run twice to show CDF dedup behavior intra-batch
COPY INTO cdf_demo_bronze_sensors
FROM (SELECT 
      id::bigint AS Id,
      device_id::integer AS device_id,
      user_id::integer AS user_id,
      calories_burnt::decimal(10,2) AS calories_burnt, 
      miles_walked::decimal(10,2) AS miles_walked, 
      num_steps::decimal(10,2) AS num_steps, 
      timestamp::timestamp AS event_timestamp,
      value  AS value, -- This is a JSON object,
      now() AS bronze_update_timestamp
FROM "/databricks-datasets/iot-stream/data-device/")
FILEFORMAT = json -- csv, xml, txt, parquet, binary, etc.
COPY_OPTIONS('force'='true', 'ignoreChanges' = 'true', 'ignoreDeletes' = 'true', 'mergeSchema' = 'true')


-- COMMAND ----------

-- DBTITLE 1,Insert / Delete Wacky Records to Show CDF change intra batch
-- We can inject strange behavior and then check downstream if properly propagated

UPDATE cdf_demo_bronze_sensors 
SET num_steps = 99999
WHERE Id = 1;


DELETE FROM cdf_demo_bronze_sensors
WHERE Id = 2;

